# -*- coding: utf-8 -*-
"""first_steps_with_tensor_flow.ipynb

Automatically generated by Colaboratory.



# Getting started with TensorFlow

** Learning objectives: **
   * Basic concepts of TensorFlow
   * Use TensorFlow's LinearRegressor class to predict the median house price, at the city block level, based on a single input characteristic
   * Evaluate the accuracy of a model's predictions using RMSE
   * Improve the accuracy of a model by modifying its hyperparameters
Data is based on the 1990 California State Census.

## Configuration
#Download needed packages
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function

import math

from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
# %tensorflow_version 1.x
import tensorflow as tf
from tensorflow.python.data import Dataset

tf.logging.set_verbosity(tf.logging.ERROR)
pd.options.display.max_rows = 10
pd.options.display.float_format = '{:.1f}'.format

"""#Download California housing dataset"""

california_df = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv", sep=",")
california_df.head()

"""The data are mixed randomly to avoid pathological classification effects which can adversely affect the performance of the stochastic gradient descent.

We are also going to scale the 'median_house_value' value so that it is expressed in thousands.It is a easier to learn with rates in the range generally used.
"""

california_df = california_housing_dataframe.reindex(
    np.random.permutation(california_df.index))
california_housing_dataframe["median_house_value"] /= 1000.0
california_housing_dataframe

""" ## Data Analysis

We will print a brief summary of some useful statistics on each column: number of examples, mean, standard deviation, maximum, minimum and various quantiles.


"""

california_housing_dataframe.describe()

"""## First model
We will try to predict the median value of a dwelling (median_house_value) which will become the label (which is sometimes also referred to as the target).We will use the number of rooms (total_rooms) as the input characteristic.

To train the model, we'll use the [LinearRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor)interface provided by TensorFlow's [Estimator](https://www.tensorflow.org/get_started/estimator)API.

This API takes care of mechanics of the low-level model, and provides convenient methods for performing model learning, evaluation, and inference tasks.

Step 1: Configure characteristic columns

In order to be able to import the training data into TensorFlow, we need to specify the type of data that is in each characteristic. We will mainly use two types of data:

1 / Categorical data: this is textual data. The real estate dataset used does not contain any categorical characteristics. This type of data could be the style of accommodation, the content of a real estate ad, etc.

2 / Numerical data: data representing a number (integer or floating point). As we will see later, in some cases you can treat numeric data (a postal code, for example), as if it were categorical.

In TensorFlow, the data type of a characteristic is specified using a construct called a characteristic column. Columns of this type only store a description of the characteristic's data.

First, we'll just use a 'total_rooms' numeric input characteristic.
 The following code extracts the total_rooms data from the california_housing_dataframe set and sets the characteristics column using numeric_column, which specifies that its data is numeric:
"""

# Define the input feature: total_rooms.
my_feature = california_housing_dataframe[["total_rooms"]]

# Configure a numeric feature column for total_rooms.
feature_columns = [tf.feature_column.numeric_column("total_rooms")]

"""#Step 2: Define the target

We will then define the target, namely the median value of a house (median_house_value). 
"""

# Define the label.
targets = california_housing_dataframe["median_house_value"]
targets

"""Step 3: Configure the LinearRegressor Class

Next, we'll set up a linear regression model using LinearRegressor. We will train this model using GradientDescentOptimizer, which implements stochastic gradient descent in mini-batches. The learning_rate argument determines the size of the gradient step.

 https://developers.google.com/machine-learning/glossary/#gradient_clipping 
"""

# Use gradient descent as the optimizer for training the model.
my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0000001)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)

# Configure the linear regression model with our feature columns and optimizer.
# Set a learning rate of 0.0000001 for Gradient Descent.
linear_regressor = tf.estimator.LinearRegressor(
    feature_columns=feature_columns,
    optimizer=my_optimizer
)

"""Step 4: Define the input function

To import California state real estate data into LinearRegressor, an input function must be defined. This function not only tells TensorFlow how to preprocess the data, but also how to batch, read in random mode, and repeat it while training the model.

NOTE: When the default value of num_epochs = None is passed to repeat (), the input data is repeated indefinitely.

Then, if shuffle is set to True, the data will be read randomly, so that it is passed randomly to the model during training. The buffer_size argument indicates the size of the dataset from which the shuffle will be randomly sampled.

Finally, the input function builds an iterator for the dataset and returns the next dataset to LinearRegressor.
"""

def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):
    """Trains a linear regression model of one feature.
  
    Args:
      features: pandas DataFrame of features
      targets: pandas DataFrame of targets
      batch_size: Size of batches to be passed to the model
      shuffle: True or False. Whether to shuffle the data.
      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely
    Returns:
      Tuple of (features, labels) for next data batch
    """
  
    # Convert pandas data into a dict of np arrays.
    features = {key:np.array(value) for key,value in dict(features).items()}                                           
 
    # Construct a dataset, and configure batching/repeating.
    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit
    ds = ds.batch(batch_size).repeat(num_epochs)
    
    # Shuffle the data, if specified.
    if shuffle:
      ds = ds.shuffle(buffer_size=10000)
    
    # Return the next batch of data.
    features, labels = ds.make_one_shot_iterator().get_next()
    return features, labels

"""NOTE: For more information on input functions and the Dataset API, see the [TensorFlow Programmer's Guide] (https://www.tensorflow.org/programmers_guide/datasets).

Step 5: Train the model

We will wrap my_input_fn in a lambda so that we can pass my_feature and target as arguments (for more information, see this tutorial on the TensorFlow input function).
To begin with, you will learn for 100 steps.

https://www.tensorflow.org/get_started/input_fn#passing_input_fn_data_to_your_model
"""

_ = linear_regressor.train(
    input_fn = lambda:my_input_fn(my_feature, targets),
    steps=100
)

"""Step 6: Evaluate the model

We are going to make predictions on the training data to determine how well it fits our model during training.

NOTE: Training error measures how well your model fits the training data. However, it does not measure the quality of generalization of the model to new data.
"""

# Create an input function for predictions.
# Note: Since we're making just one prediction for each example, we don't 
# need to repeat or shuffle the data here.
prediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)

# Call predict() on the linear_regressor to make predictions.
predictions = linear_regressor.predict(input_fn=prediction_input_fn)

# Format predictions as a NumPy array, so we can calculate error metrics.
predictions = np.array([item['predictions'][0] for item in predictions])

# Print Mean Squared Error and Root Mean Squared Error.
mean_squared_error = metrics.mean_squared_error(predictions, targets)
root_mean_squared_error = math.sqrt(mean_squared_error)
print("Mean Squared Error (on training data): %0.3f" % mean_squared_error)
print("Root Mean Squared Error (on training data): %0.3f" % root_mean_squared_error)

"""Is this a good model? How to assess the importance of the error?

Since mean squared error (MSE) can be difficult to interpret, the square root of the mean square error (RMSE) is considered instead. An interesting property of RMSE is the ability to interpret it on the same scale as the original targets.

Let's compare the RMSE to the difference between the minimum and maximum values of our targets:
"""

min_house_value = california_housing_dataframe["median_house_value"].min()
max_house_value = california_housing_dataframe["median_house_value"].max()
min_max_difference = max_house_value - min_house_value

print("Min. Median House Value: %0.3f" % min_house_value)
print("Max. Median House Value: %0.3f" % max_house_value)
print("Difference between Min. and Max.: %0.3f" % min_max_difference)
print("Root Mean Squared Error: %0.3f" % root_mean_squared_error)

"""The error covers almost half of the range of target values. Can we do better?

This is the question that annoys all model developers. Let’s work out some basic strategies to reduce model error.

To get started, you can look at how well the predictions meet the targets that have been set, in terms of overall summary statistics.
"""

calibration_data = pd.DataFrame()
calibration_data["predictions"] = pd.Series(predictions)
calibration_data["targets"] = pd.Series(targets)
calibration_data.describe()

"""This information can be very useful. How do you compare the average value to the model's RMSE value? What about the various quantiles?

You can also view the data and the row that has been learned. As a reminder, linear regression on a single characteristic can be represented by a straight line transforming the input x into the output y.

First, you'll get a uniform random sample of the data, so that you create a readable scatter plot.
"""

sample = california_housing_dataframe.sample(n=300)

"""We will then how to represent in graphical form, the line that has been learned, starting from the feature weighting and the model bias, superimposed on the scatter plot. The line will be displayed in red."""

# Get the min and max total_rooms values.
x_0 = sample["total_rooms"].min()
x_1 = sample["total_rooms"].max()

# Retrieve the final weight and bias generated during training.
weight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]
bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')

# Get the predicted median_house_values for the min and max total_rooms values.
y_0 = weight * x_0 + bias 
y_1 = weight * x_1 + bias

# Plot our regression line from (x_0, y_0) to (x_1, y_1).
plt.plot([x_0, x_1], [y_0, y_1], c='r')

# Label the graph axes.
plt.ylabel("median_house_value")
plt.xlabel("total_rooms")

# Plot a scatter plot from our data sample.
plt.scatter(sample["total_rooms"], sample["median_house_value"])

# Display graph.
plt.show()

"""This initial line seems very far away. See if it is possible to go back to the summary statistics and examine the same information encoded there.

These integrity assessments suggest that it must be possible to find a much better line.

## Modify model hyperparameters

All of the above code has been placed in a single function for convenience. You can call this function with different parameters to view the effects.

In this function, we will work in 10 periods distributed evenly, in order to be able to observe the improvement of the model in each period.

For each period, we will calculate and graph the learning loss. This can help us determine if a model has converged or if additional iterations are needed.

We will also graph the bias and feature weight values learned by the model over time. This is another method to visualize the convergence of elements.
"""

def train_model(learning_rate, steps, batch_size, input_feature="total_rooms"):
  """Trains a linear regression model of one feature.
  
  Args:
    learning_rate: A `float`, the learning rate.
    steps: A non-zero `int`, the total number of training steps. A training step
      consists of a forward and backward pass using a single batch.
    batch_size: A non-zero `int`, the batch size.
    input_feature: A `string` specifying a column from `california_housing_dataframe`
      to use as input feature.
  """
  
  periods = 10
  steps_per_period = steps / periods

  my_feature = input_feature
  my_feature_data = california_housing_dataframe[[my_feature]]
  my_label = "median_house_value"
  targets = california_housing_dataframe[my_label]

  # Create feature columns.
  feature_columns = [tf.feature_column.numeric_column(my_feature)]
  
  # Create input functions.
  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)
  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)
  
  # Create a linear regressor object.
  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
  linear_regressor = tf.estimator.LinearRegressor(
      feature_columns=feature_columns,
      optimizer=my_optimizer
  )

  # Set up to plot the state of our model's line each period.
  plt.figure(figsize=(15, 6))
  plt.subplot(1, 2, 1)
  plt.title("Learned Line by Period")
  plt.ylabel(my_label)
  plt.xlabel(my_feature)
  sample = california_housing_dataframe.sample(n=300)
  plt.scatter(sample[my_feature], sample[my_label])
  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]

  # Train the model, but do so inside a loop so that we can periodically assess
  # loss metrics.
  print("Training model...")
  print("RMSE (on training data):")
  root_mean_squared_errors = []
  for period in range (0, periods):
    # Train the model, starting from the prior state.
    linear_regressor.train(
        input_fn=training_input_fn,
        steps=steps_per_period
    )
    # Take a break and compute predictions.
    predictions = linear_regressor.predict(input_fn=prediction_input_fn)
    predictions = np.array([item['predictions'][0] for item in predictions])
    
    # Compute loss.
    root_mean_squared_error = math.sqrt(
        metrics.mean_squared_error(predictions, targets))
    # Occasionally print the current loss.
    print("  period %02d : %0.2f" % (period, root_mean_squared_error))
    # Add the loss metrics from this period to our list.
    root_mean_squared_errors.append(root_mean_squared_error)
    # Finally, track the weights and biases over time.
    # Apply some math to ensure that the data and line are plotted neatly.
    y_extents = np.array([0, sample[my_label].max()])
    
    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]
    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')

    x_extents = (y_extents - bias) / weight
    x_extents = np.maximum(np.minimum(x_extents,
                                      sample[my_feature].max()),
                           sample[my_feature].min())
    y_extents = weight * x_extents + bias
    plt.plot(x_extents, y_extents, color=colors[period]) 
  print("Model training finished.")

  # Output a graph of loss metrics over periods.
  plt.subplot(1, 2, 2)
  plt.ylabel('RMSE')
  plt.xlabel('Periods')
  plt.title("Root Mean Squared Error vs. Periods")
  plt.tight_layout()
  plt.plot(root_mean_squared_errors)

  # Output a table with calibration data.
  calibration_data = pd.DataFrame()
  calibration_data["predictions"] = pd.Series(predictions)
  calibration_data["targets"] = pd.Series(targets)
  display.display(calibration_data.describe())

  print("Final RMSE (on training data): %0.2f" % root_mean_squared_error)

"""##Task 1: Obtain an RMSE value less than or equal to 180
Modify the hyperparameters of the model to improve the cost and get a better match with the target distribution. If after about five minutes you are still unable to get an RMSE value of 180, check the solution to see an applicable combination.
"""

train_model(
    learning_rate=0.00001,
    steps=100,
    batch_size=1
)

train_model(
    learning_rate=0.00002,
    steps=500,
    batch_size=5
)

"""This is just one configuration among many; other combinations of parameters can also give good results. Note that the goal of this exercise is not to find the optimal setting, but to help you see how tuning the model configuration affects the quality of the prediction.

##Is there a standard heuristic method for model tuning?
This is a common question. In short, we can say that the effects of different hyperparameters depend on the data. So there are no hard and fast rules. You need to test the data!

That being said, here are a few rules of thumb that may come in handy:

The learning error should decrease steadily, steeply at first, and eventually stabilize as the learning converges.
If the learning hasn't converged, try to run it longer.
If the learning error decreases too slowly, increasing the learning rate may help speed the decrease.
However, sometimes the reverse happens if the learning rate is too high.
If the learning error varies significantly, try to decrease the learning rate.
A good method is usually to decrease the learning rate while increasing the number of steps or the batch size.
Very small batches can also cause instability. Start with values ​​such as 100 or 1000, and continue reducing the size until you see degradation.
As a reminder, you shouldn't follow these rules of thumb to the letter, as the effects depend on the data. You should always try a method and then check the result.

##Task 2: Try another feature
Let's see if it is possible to obtain a better result by replacing the characteristic total_rooms by population.

### Solution

Cliquez ci-dessous pour afficher une solution.
"""

train_model(
    learning_rate=0.00002,
    steps=1000,
    batch_size=5,
    input_feature="population"
)
